Namespace(all_in_cos=False, batch_size=512, cached=True, cpuonly=False, dataset='imagenet', downloadall=False, end=10000, freeze=True, freeze_idx=17, model='vit', num_epochs=1, sequential=False, split_choice='automatic', split_idx=100, start=0, testonly=False, transformed=False, use_intermediate=False)
Nb GPUS  2
==> Preparing data..
==> Building model..
Using split index: 100
Freezing the lower layers of the model (vit) till index 17
The mode is:  vanilla
Start 0, end 512, post_step 128

Memory occpied: (1630.0, 3.0)
Memory occpied: (1630.0, 3.0)
Decompress data took 0.010092973709106445 seconds
Total decompress data took 0.44962382316589355 seconds
Decompress data took 0.008826017379760742 seconds
Memory occpied: (1630.0, 3.0)
Total decompress data took 0.44371652603149414 seconds
Decompress data took 0.010129451751708984 seconds
Total decompress data took 0.41312575340270996 seconds
Decompress data took 0.00909280776977539 seconds
Memory occpied: (1630.0, 3.0)
Total decompress data took 0.49643468856811523 seconds
Read 63.78047180175781 MBs for this batch
Streaming imagenet data took 3.7649953365325928 seconds
The mode is:  vanilla
Start 512, end 1024, post_step 128


Epoch: 0
Decompress data took 0.014311075210571289 seconds
Memory occpied: (1630.0, 3.0)
Total decompress data took 0.624600887298584 seconds
Decompress data took 0.009315013885498047 seconds
Time of next(dataloader) is: 1.8965175151824951
Total decompress data took 0.48775386810302734 seconds
Decompress data took 0.009225606918334961 seconds
Time for copying to cuda: 0.07582521438598633
Memory occpied: (1924.0, 302.0)
Total decompress data took 0.5329909324645996 seconds
Decompress data took 0.009975910186767578 seconds
Total decompress data took 0.41673946380615234 seconds
Read 66.69892597198486 MBs for this batch
Streaming imagenet data took 2.861687421798706 seconds
Memory occpied: (1924.0, 734.0)
Memory occpied: (1924.0, 1228.0)
Memory occpied: (12552.0, 2050.0)
Memory occpied: (14798.0, 14922.0)
Exception: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/timm/models/vision_transformer.py", line 433, in forward
    x = self.forward_features(x)
  File "/usr/local/lib/python3.8/dist-packages/timm/models/vision_transformer.py", line 421, in forward_features
    x = self.blocks(x)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/timm/models/vision_transformer.py", line 238, in forward
    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/timm/models/vision_transformer.py", line 199, in forward
    attn = (q @ k.transpose(-2, -1)) * self.scale
RuntimeError: CUDA out of memory. Tried to allocate 456.00 MiB (GPU 0; 14.76 GiB total capacity; 12.54 GiB already allocated; 311.75 MiB free; 13.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The whole process took 17.783722162246704 seconds
