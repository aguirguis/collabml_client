Namespace(all_in_cos=False, batch_size=5000, cached=True, cpuonly=False, dataset='imagenet', downloadall=False, end=10000, freeze=True, freeze_idx=17, model='myalexnet', num_epochs=1, sequential=False, split_choice='automatic', split_idx=100, start=0, testonly=False, transformed=True, use_intermediate=True)
Nb GPUS  2
==> Preparing data..
==> Building model..
IN SPLITTING ALGO
Recorded bandwidth: 907.6278946991396 Mbps
In _get_intermediate_outputs_and_time
Done intermediate outputs and time
Sizes  [774400. 774400. 186624. 559872. 559872. 129792. 259584. 259584. 173056.
 173056. 173056. 173056.  36864.  36864.  36864.  16384.  16384.  16384.
  16384.  16384.   4000.]
Input_size  0.57421875
TESTING *****************************
Input size, BW, MIN:
448967248008533.3 118964603.41400562 118964603.41400562
All candidates indexes:  (array([15, 16, 17, 18, 19, 20]),)
SPLIT IDX CHOICE, split idx manual, freeze_idx:  automatic 100 17
Intermediate:  0.015625
1.47705078125
Total layers size  4.189361572265625
Server, client, server+client, vanilla  299.09765625 974.72900390625 1273.82666015625 11152.95166015625
Candidate split  16
Server, client, server+client, vanilla  299.09765625 974.72900390625 1273.82666015625 11152.95166015625
Model size  233.45703125
Fixed, scale_with_bsz  233.45703125 2.05126953125
Mem usage  1514.0 3.0
Using split index: 16
Freezing the lower layers of the model (myalexnet) till index 17
The mode is:  split
Start 0, end 5000, post_step 1000

Memory occpied: (1514.0, 3.0)
Memory occpied: (1514.0, 3.0)
Memory occpied: (1514.0, 3.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.677844285964966 seconds
Streaming imagenet data took 2.708571434020996 seconds
The mode is:  split
Start 5000, end 10000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.397202730178833
Time for copying to cuda: 0.020989418029785156
Memory occpied: (1594.0, 316.0)
Memory occpied: (1594.0, 738.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.1226582527160645 seconds
Streaming imagenet data took 3.1538758277893066 seconds
Memory occpied: (1594.0, 1190.0)
Time for forward pass: 3.3328347206115723
Time for backpropagation: 0.05601859092712402
GPU memory for training: 1.1339058876037598                          

One training iteration takes: 3.8831019401550293 seconds
Index: 0
Then, training+dataloading take 3.88340163230896 seconds
The mode is:  split
Start 10000, end 15000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.4015352725982666
Time for copying to cuda: 0.020758867263793945
Time for forward pass: 0.05975508689880371
Time for backpropagation: 0.0034308433532714844
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.600250244140625 seconds
Index: 1
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.1318511962890625 seconds
Streaming imagenet data took 3.208789587020874 seconds
Then, training+dataloading take 3.2248785495758057 seconds
The mode is:  split
Start 15000, end 20000, post_step 1000


Epoch: 0
Memory occpied: (2494.0, 1788.0)
Time of next(dataloader) is: 0.45984387397766113
Time for copying to cuda: 0.020302534103393555
Time for forward pass: 0.06363821029663086
Time for backpropagation: 0.002850055694580078
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.6355986595153809 seconds
Index: 2
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.998211622238159 seconds
Streaming imagenet data took 3.028748035430908 seconds
Then, training+dataloading take 3.0308241844177246 seconds
The mode is:  split
Start 20000, end 25000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.42230677604675293
Time for copying to cuda: 0.02048659324645996
Time for forward pass: 0.09784960746765137
Time for backpropagation: 0.003213644027709961
GPU memory for training: 1.2909655570983887                          

Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.087775945663452 seconds
Streaming imagenet data took 3.1622540950775146 seconds
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
One training iteration takes: 5.604436159133911 seconds
Index: 3
Then, training+dataloading take 5.6048619747161865 seconds
The mode is:  split
Start 25000, end 30000, post_step 1000


Epoch: 0
Memory occpied: (2494.0, 1788.0)
Time of next(dataloader) is: 0.4359707832336426
Time for copying to cuda: 0.0210726261138916
Time for forward pass: 0.06355071067810059
Time for backpropagation: 0.002712249755859375
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.6300787925720215 seconds
Index: 4
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.0989177227020264 seconds
Streaming imagenet data took 3.1298818588256836 seconds
Then, training+dataloading take 3.134147882461548 seconds
The mode is:  split
Start 30000, end 32000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.4580986499786377
Time for copying to cuda: 0.020467519760131836
Time for forward pass: 0.08991837501525879
Time for backpropagation: 0.003247976303100586
GPU memory for training: 1.2909655570983887                          

Memory occpied: (2494.0, 1788.0)
One training iteration takes: 0.6724898815155029 seconds
Index: 5
Memory occpied: (2494.0, 1788.0)
Read 31.319355010986328 MBs for this batch
Executing all posts took 2.0870234966278076 seconds
Streaming imagenet data took 2.1015071868896484 seconds
Then, training+dataloading take 2.102612257003784 seconds

Epoch: 0
Time of next(dataloader) is: 0.41276001930236816
Time for copying to cuda: 0.008537054061889648
Time for forward pass: 0.03909444808959961
Time for backpropagation: 0.002726316452026367
GPU memory for training: 1.1563701629638672                          

The whole process took 30.998905181884766 seconds
