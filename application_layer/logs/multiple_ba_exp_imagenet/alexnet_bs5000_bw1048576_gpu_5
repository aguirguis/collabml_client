Cached  True
Transformed  True
All in COS  False
No adaptation  False
Namespace(all_in_cos=False, batch_size=5000, cached=True, cpuonly=False, dataset='imagenet', downloadall=False, end=10000, freeze=True, freeze_idx=17, model='myalexnet', no_adaptation=False, num_epochs=1, sequential=False, split_choice='automatic', split_idx=100, start=0, testonly=False, transformed=True, use_intermediate=True)
Nb GPUS  2
==> Preparing data..
==> Building model..
IN SPLITTING ALGO
Recorded bandwidth: 908.5844829102763 Mbps
In _get_intermediate_outputs_and_time
Done intermediate outputs and time
Sizes  [774400. 774400. 186624. 559872. 559872. 129792. 259584. 259584. 173056.
 173056. 173056. 173056.  36864.  36864.  36864.  16384.  16384.  16384.
  16384.  16384.   4000.]
Input_size  0.57421875
TESTING *****************************
Input size, BW, MIN:
448967248008533.3 119089985.34401573 119089985.34401573
All candidates indexes:  (array([15, 16, 17, 18, 19, 20]),)
SPLIT IDX CHOICE, split idx manual, freeze_idx:  automatic 100 17
Intermediate:  0.015625
1.47705078125
Total layers size  4.189361572265625
Server, client, server+client, vanilla  299.09765625 974.72900390625 1273.82666015625 11152.95166015625
Candidate split  16
Server, client, server+client, vanilla  299.09765625 974.72900390625 1273.82666015625 11152.95166015625
Model size  233.45703125
Fixed, scale_with_bsz  233.45703125 2.05126953125
Mem usage  1514.0 3.0
Using split index: 16
Freezing the lower layers of the model (myalexnet) till index 17
The mode is:  split
Start 0, end 5000, post_step 1000

Memory occpied: (1514.0, 3.0)
Memory occpied: (1514.0, 3.0)
Memory occpied: (1514.0, 3.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.718050003051758 seconds
Streaming imagenet data took 2.7488536834716797 seconds
The mode is:  split
Start 5000, end 10000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.3498494625091553
Time for copying to cuda: 0.02068352699279785
Memory occpied: (1594.0, 308.0)
Memory occpied: (1594.0, 706.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.5168676376342773 seconds
Streaming imagenet data took 2.548232316970825 seconds
Memory occpied: (1594.0, 1168.0)
Time for forward pass: 3.276641845703125
Time for backpropagation: 0.05504918098449707
GPU memory for training: 1.1339058876037598                          

One training iteration takes: 3.7860188484191895 seconds
Index: 0
Then, training+dataloading take 3.786306381225586 seconds
The mode is:  split
Start 10000, end 15000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.37681007385253906
Time for copying to cuda: 0.020721912384033203
Time for forward pass: 0.06332921981811523
Time for backpropagation: 0.0036478042602539062
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.5431275367736816 seconds
Index: 1
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.6358156204223633 seconds
Streaming imagenet data took 3.6672582626342773 seconds
Then, training+dataloading take 3.669710159301758 seconds
The mode is:  split
Start 15000, end 20000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.38950014114379883
Time for copying to cuda: 0.020272254943847656
Time for forward pass: 0.0669102668762207
Time for backpropagation: 0.003202676773071289
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.5613851547241211 seconds
Index: 2
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.541071891784668 seconds
Streaming imagenet data took 2.5721399784088135 seconds
Then, training+dataloading take 2.5741541385650635 seconds
The mode is:  split
Start 20000, end 25000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.42551660537719727
Time for copying to cuda: 0.05796241760253906
Memory occpied: (2494.0, 1788.0)
Time for forward pass: 0.06380224227905273
Time for backpropagation: 0.0027577877044677734
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.6327469348907471 seconds
Index: 3
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.4559431076049805 seconds
Streaming imagenet data took 3.487743616104126 seconds
Then, training+dataloading take 3.490373134613037 seconds
The mode is:  split
Start 25000, end 30000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.3859834671020508
Time for copying to cuda: 0.02022719383239746
Time for forward pass: 0.09183382987976074
Time for backpropagation: 0.0036325454711914062
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.5886406898498535 seconds
Index: 4
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.60880184173584 seconds
Streaming imagenet data took 2.639763355255127 seconds
Then, training+dataloading take 2.6418070793151855 seconds
The mode is:  split
Start 30000, end 32000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.4304203987121582
Time for copying to cuda: 0.02030158042907715
Time for forward pass: 0.06352591514587402
Time for backpropagation: 0.0027353763580322266
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.6378586292266846 seconds
Index: 5
Memory occpied: (2494.0, 1788.0)
Read 31.319355010986328 MBs for this batch
Executing all posts took 1.798492193222046 seconds
Streaming imagenet data took 1.8501570224761963 seconds
Then, training+dataloading take 1.856781244277954 seconds

Epoch: 0
Memory occpied: (2494.0, 1788.0)
Time of next(dataloader) is: 0.3509812355041504
Time for copying to cuda: 0.008416414260864258
Time for forward pass: 0.03891277313232422
Time for backpropagation: 0.0025930404663085938
GPU memory for training: 1.1563701629638672                          

The whole process took 28.28050947189331 seconds
