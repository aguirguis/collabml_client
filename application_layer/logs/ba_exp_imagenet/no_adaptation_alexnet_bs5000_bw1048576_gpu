Cached  True
Transformed  True
All in COS  False
No adaptation  True
Namespace(all_in_cos=False, batch_size=5000, cached=True, cpuonly=False, dataset='imagenet', downloadall=False, end=10000, freeze=True, freeze_idx=17, model='myalexnet', no_adaptation=True, num_epochs=1, sequential=False, split_choice='automatic', split_idx=100, start=0, testonly=False, transformed=True, use_intermediate=True)
Nb GPUS  2
==> Preparing data..
==> Building model..
IN SPLITTING ALGO
Recorded bandwidth: 908.4637576584698 Mbps
In _get_intermediate_outputs_and_time
Done intermediate outputs and time
Sizes  [774400. 774400. 186624. 559872. 559872. 129792. 259584. 259584. 173056.
 173056. 173056. 173056.  36864.  36864.  36864.  16384.  16384.  16384.
  16384.  16384.   4000.]
Input_size  0.57421875
TESTING *****************************
Input size, BW, MIN:
448967248008533.3 119074161.64381096 119074161.64381096
All candidates indexes:  (array([15, 16, 17, 18, 19, 20]),)
SPLIT IDX CHOICE, split idx manual, freeze_idx:  automatic 100 17
Intermediate:  0.015625
1.47705078125
Total layers size  4.189361572265625
Server, client, server+client, vanilla  299.09765625 974.72900390625 1273.82666015625 11152.95166015625
Candidate split  16
Server, client, server+client, vanilla  299.09765625 974.72900390625 1273.82666015625 11152.95166015625
Model size  233.45703125
Fixed, scale_with_bsz  233.45703125 2.05126953125
Mem usage  1514.0 3.0
Using split index: 16
Freezing the lower layers of the model (myalexnet) till index 17
The mode is:  split
Start 0, end 5000, post_step 1000

Memory occpied: (1514.0, 3.0)
Memory occpied: (1514.0, 3.0)
Memory occpied: (1514.0, 3.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.835737705230713 seconds
Streaming imagenet data took 2.8666465282440186 seconds
The mode is:  split
Start 5000, end 10000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.35964369773864746
Time for copying to cuda: 0.02078104019165039
Memory occpied: (1594.0, 240.0)
Memory occpied: (1594.0, 704.0)
Memory occpied: (1594.0, 1150.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.4101386070251465 seconds
Streaming imagenet data took 3.442143201828003 seconds
Time for forward pass: 3.3383994102478027
Time for backpropagation: 0.055005550384521484
GPU memory for training: 1.1339058876037598                          

One training iteration takes: 3.85815691947937 seconds
Index: 0
Then, training+dataloading take 3.858426809310913 seconds
The mode is:  split
Start 10000, end 15000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.39078307151794434
Time for copying to cuda: 0.020824909210205078
Time for forward pass: 0.05934453010559082
Time for backpropagation: 0.0045316219329833984
GPU memory for training: 1.2909655570983887                          

Memory occpied: (2350.0, 1788.0)
One training iteration takes: 0.5573108196258545 seconds
Index: 1
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.0820772647857666 seconds
Streaming imagenet data took 3.112936019897461 seconds
Then, training+dataloading take 3.1169207096099854 seconds
The mode is:  split
Start 15000, end 20000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.38831329345703125
Time for copying to cuda: 0.020606040954589844
Time for forward pass: 0.06346249580383301
Time for backpropagation: 0.0027523040771484375
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.5559060573577881 seconds
Index: 2
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.325714588165283 seconds
Streaming imagenet data took 3.3577170372009277 seconds
Then, training+dataloading take 3.3734211921691895 seconds
The mode is:  split
Start 20000, end 25000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.44383716583251953
Time for copying to cuda: 0.020449399948120117
Time for forward pass: 0.06352519989013672
Time for backpropagation: 0.002759695053100586
GPU memory for training: 1.2909655570983887                          

Memory occpied: (2494.0, 1788.0)
One training iteration takes: 0.6215393543243408 seconds
Index: 3
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 3.033780813217163 seconds
Memory occpied: (2494.0, 1788.0)
Streaming imagenet data took 3.0655276775360107 seconds
Then, training+dataloading take 3.0697062015533447 seconds
The mode is:  split
Start 25000, end 30000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.39433956146240234
Time for copying to cuda: 0.02039170265197754
Time for forward pass: 0.06346416473388672
Time for backpropagation: 0.0026700496673583984
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.5735998153686523 seconds
Index: 4
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.9059855937957764 seconds
Streaming imagenet data took 2.9378421306610107 seconds
Then, training+dataloading take 2.9398255348205566 seconds
The mode is:  split
Start 30000, end 32000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.3954124450683594
Time for copying to cuda: 0.020205259323120117
Time for forward pass: 0.07804250717163086
Time for backpropagation: 0.003262042999267578
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.5806388854980469 seconds
Index: 5
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 31.319355010986328 MBs for this batch
Executing all posts took 2.0368340015411377 seconds
Streaming imagenet data took 2.0516350269317627 seconds
Then, training+dataloading take 2.052757501602173 seconds

Epoch: 0
Time of next(dataloader) is: 0.3656015396118164
Time for copying to cuda: 0.00841832160949707
Time for forward pass: 0.038961172103881836
Time for backpropagation: 0.0026803016662597656
GPU memory for training: 1.1563701629638672                          

The whole process took 28.61238718032837 seconds
