Cached  True
Transformed  True
All in COS  False
No adaptation  True
Namespace(all_in_cos=False, batch_size=5000, cached=True, cpuonly=False, dataset='imagenet', downloadall=False, end=10000, freeze=True, freeze_idx=17, model='myalexnet', no_adaptation=True, num_epochs=1, sequential=False, split_choice='automatic', split_idx=100, start=0, testonly=False, transformed=True, use_intermediate=True)
Nb GPUS  2
==> Preparing data..
==> Building model..
IN SPLITTING ALGO
Recorded bandwidth: 907.5013913967301 Mbps
In _get_intermediate_outputs_and_time
Done intermediate outputs and time
Sizes  [774400. 774400. 186624. 559872. 559872. 129792. 259584. 259584. 173056.
 173056. 173056. 173056.  36864.  36864.  36864.  16384.  16384.  16384.
  16384.  16384.   4000.]
Input_size  0.57421875
TESTING *****************************
Input size, BW, MIN:
224483624004266.66 118948022.37315221 118948022.37315221
All candidates indexes:  (array([15, 16, 17, 18, 19, 20]),)
SPLIT IDX CHOICE, split idx manual, freeze_idx:  automatic 100 17
Intermediate:  0.015625
1.47705078125
Total layers size  4.189361572265625
Server, client, server+client, vanilla  266.27734375 974.72900390625 1241.00634765625 11152.95166015625
Candidate split  16
Server, client, server+client, vanilla  266.27734375 974.72900390625 1241.00634765625 11152.95166015625
Model size  233.45703125
Fixed, scale_with_bsz  233.45703125 2.05126953125
Mem usage  1514.0 3.0
Using split index: 16
Freezing the lower layers of the model (myalexnet) till index 17
The mode is:  split
Start 0, end 5000, post_step 1000

Memory occpied: (1514.0, 3.0)
Memory occpied: (1514.0, 3.0)
Memory occpied: (1514.0, 3.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.8770761489868164 seconds
Streaming imagenet data took 2.9078867435455322 seconds
The mode is:  split
Start 5000, end 10000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.3666958808898926
Time for copying to cuda: 0.02083730697631836
Memory occpied: (1594.0, 208.0)
Memory occpied: (1594.0, 674.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.870389223098755 seconds
Streaming imagenet data took 2.902240037918091 seconds
Memory occpied: (1594.0, 1112.0)
Time for forward pass: 3.3494174480438232
Time for backpropagation: 0.0554356575012207
GPU memory for training: 1.1339058876037598                          

One training iteration takes: 3.8699402809143066 seconds
Index: 0
Then, training+dataloading take 3.8702120780944824 seconds
The mode is:  split
Start 10000, end 15000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.420393705368042
Time for copying to cuda: 0.020725727081298828
Time for forward pass: 0.06334900856018066
Time for backpropagation: 0.0034499168395996094
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.6163551807403564 seconds
Index: 1
Memory occpied: (2188.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.8481433391571045 seconds
Streaming imagenet data took 2.91929292678833 seconds
Then, training+dataloading take 2.9352469444274902 seconds
The mode is:  split
Start 15000, end 20000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.4554262161254883
Time for copying to cuda: 0.020505905151367188
Time for forward pass: 0.06359267234802246
Time for backpropagation: 0.0029566287994384766
GPU memory for training: 1.2909655570983887                          

Memory occpied: (2494.0, 1788.0)
One training iteration takes: 0.6302127838134766 seconds
Index: 2
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.3957059383392334 seconds
Streaming imagenet data took 2.427546501159668 seconds
Then, training+dataloading take 2.429654121398926 seconds
The mode is:  split
Start 20000, end 25000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.42703771591186523
Time for copying to cuda: 0.020395278930664062
Time for forward pass: 0.0814056396484375
Time for backpropagation: 0.003367185592651367
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.6123020648956299 seconds
Index: 3
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.4417476654052734 seconds
Streaming imagenet data took 2.4729626178741455 seconds
Then, training+dataloading take 2.4756107330322266 seconds
The mode is:  split
Start 25000, end 30000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.39353203773498535
Time for copying to cuda: 0.020508766174316406
Time for forward pass: 0.08223366737365723
Time for backpropagation: 0.003371000289916992
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.5904707908630371 seconds
Index: 4
Memory occpied: (2494.0, 1788.0)
Memory occpied: (2494.0, 1788.0)
Read 78.29838752746582 MBs for this batch
Executing all posts took 2.5148463249206543 seconds
Streaming imagenet data took 2.5455617904663086 seconds
Then, training+dataloading take 2.547497510910034 seconds
The mode is:  split
Start 30000, end 32000, post_step 1000


Epoch: 0
Time of next(dataloader) is: 0.45964932441711426
Time for copying to cuda: 0.020577430725097656
Time for forward pass: 0.06363272666931152
Time for backpropagation: 0.002937793731689453
GPU memory for training: 1.2909655570983887                          

One training iteration takes: 0.6655058860778809 seconds
Index: 5
Memory occpied: (2494.0, 1788.0)
Read 31.319355010986328 MBs for this batch
Executing all posts took 1.606477975845337 seconds
Streaming imagenet data took 1.6191706657409668 seconds
Then, training+dataloading take 1.6202702522277832 seconds

Epoch: 0
Time of next(dataloader) is: 0.39648938179016113
Time for copying to cuda: 0.008406877517700195
Time for forward pass: 0.03901314735412598
Time for backpropagation: 0.0027844905853271484
GPU memory for training: 1.1563701629638672                          

Memory occpied: (2494.0, 1788.0)
The whole process took 26.846034049987793 seconds
