Beginning of exp
nvidia-smi gpu utilization  3.0
torch cuda memory allocated  0.0
torch cuda max memory allocated  0.0
torch cuda memory reserved  0.0

Input size  0.02288818359375
After loading input to cuda, input batch size 1
nvidia-smi gpu utilization  1250.0
torch cuda memory allocated  0.02294921875
torch cuda max memory allocated  0.02294921875
torch cuda memory reserved  2.0

Model_size:  0.1414642333984375
After loading model to cuda 
nvidia-smi gpu utilization  1250.0
torch cuda memory allocated  0.16552734375
torch cuda max memory allocated  0.16552734375
torch cuda memory reserved  2.0

Layer sizes, total sum tensor([0.0317, 0.0317, 0.0645, 0.0645], device='cuda:0') tensor(0.1924, device='cuda:0')
Expected (input+layers+model):  tensor(0.3567, device='cuda:0')
After inference eval mode
nvidia-smi gpu utilization  1262.0
torch cuda memory allocated  0.26220703125
torch cuda max memory allocated  0.4521484375
torch cuda memory reserved  2.0

Layer sizes, total sum tensor([0.0317, 0.0317, 0.0645, 0.0645], device='cuda:0') tensor(0.1924, device='cuda:0')
Expected (input+2*layers+model):  tensor(0.5491, device='cuda:0')
After inference train mode
nvidia-smi gpu utilization  1262.0
torch cuda memory allocated  0.26220703125
torch cuda max memory allocated  0.548828125
torch cuda memory reserved  2.0


Beginning of exp
nvidia-smi gpu utilization  1262.0
torch cuda memory allocated  0.26220703125
torch cuda max memory allocated  0.548828125
torch cuda memory reserved  2.0

Input size  0.2288818359375
After loading input to cuda, input batch size 10
nvidia-smi gpu utilization  1262.0
torch cuda memory allocated  0.4912109375
torch cuda max memory allocated  0.548828125
torch cuda memory reserved  2.0

Model_size:  0.1414642333984375
After loading model to cuda 
nvidia-smi gpu utilization  1262.0
torch cuda memory allocated  0.6337890625
torch cuda max memory allocated  0.6337890625
torch cuda memory reserved  2.0

Layer sizes, total sum tensor([0.3174, 0.3174, 0.6445, 0.6445], device='cuda:0') tensor(1.9238, device='cuda:0')
Expected (input+layers+model):  tensor(2.2942, device='cuda:0')
After inference eval mode
nvidia-smi gpu utilization  1284.0
torch cuda memory allocated  1.333984375
torch cuda max memory allocated  7.61962890625
torch cuda memory reserved  24.0

Layer sizes, total sum tensor([0.3174, 0.3174, 0.6445, 0.6445], device='cuda:0') tensor(1.9238, device='cuda:0')
Expected (input+2*layers+model):  tensor(4.2180, device='cuda:0')
After inference train mode
nvidia-smi gpu utilization  1284.0
torch cuda memory allocated  1.333984375
torch cuda max memory allocated  8.31982421875
torch cuda memory reserved  24.0


Beginning of exp
nvidia-smi gpu utilization  1264.0
torch cuda memory allocated  1.333984375
torch cuda max memory allocated  8.31982421875
torch cuda memory reserved  4.0

Input size  2.288818359375
After loading input to cuda, input batch size 100
nvidia-smi gpu utilization  1284.0
torch cuda memory allocated  3.623046875
torch cuda max memory allocated  8.31982421875
torch cuda memory reserved  24.0

Model_size:  0.1414642333984375
After loading model to cuda 
nvidia-smi gpu utilization  1284.0
torch cuda memory allocated  3.765625
torch cuda max memory allocated  8.31982421875
torch cuda memory reserved  24.0

Layer sizes, total sum tensor([3.1738, 3.1738, 6.4453, 6.4453], device='cuda:0') tensor(19.2383, device='cuda:0')
Expected (input+layers+model):  tensor(21.6686, device='cuda:0')
After inference eval mode
nvidia-smi gpu utilization  1326.0
torch cuda memory allocated  12.05126953125
torch cuda max memory allocated  53.560546875
torch cuda memory reserved  66.0

Layer sizes, total sum tensor([3.1738, 3.1738, 6.4453, 6.4453], device='cuda:0') tensor(19.2383, device='cuda:0')
Expected (input+2*layers+model):  tensor(40.9068, device='cuda:0')
After inference train mode
nvidia-smi gpu utilization  1368.0
torch cuda memory allocated  12.05126953125
torch cuda max memory allocated  61.84619140625
torch cuda memory reserved  108.0


